failed: 15
results:
- error: could not broadcast input array from shape (40,) into shape (62,)
  model_id: model_001
  model_path: null
  params:
    algorithm: ppo
    batch_size: 128
    clip_range: 0.3
    ent_coef: 0.0
    gamma: 0.995
    initial_capital: 100000
    learning_rate: 0.001
    model_id: model_001
    n_steps: 4096
    name: ppo_model_001
    reward_scaling: 1e-5
    tickers:
    - AAPL
    - GOOGL
    - AMZN
    - MSFT
    - NVDA
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 311, in learn\n    total_timesteps, callback = self._setup_learn(\n  File\
    \ \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py\"\
    , line 423, in _setup_learn\n    self._last_obs = self.env.reset()  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 79, in reset\n    self._save_obs(env_idx, obs)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 109, in _save_obs\n    self.buf_obs[key][env_idx] = obs\nValueError: could\
    \ not broadcast input array from shape (40,) into shape (62,)\n"
  training_time: 3.977125
- error: could not broadcast input array from shape (49,) into shape (38,)
  model_id: model_002
  model_path: null
  params:
    algorithm: ppo
    batch_size: 128
    clip_range: 0.3
    ent_coef: 0.0
    gamma: 0.995
    initial_capital: 200000
    learning_rate: 0.001
    model_id: model_002
    n_steps: 4096
    name: ppo_model_002
    reward_scaling: 1e-5
    tickers:
    - AAPL
    - MSFT
    - GOOGL
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 311, in learn\n    total_timesteps, callback = self._setup_learn(\n  File\
    \ \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py\"\
    , line 423, in _setup_learn\n    self._last_obs = self.env.reset()  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 79, in reset\n    self._save_obs(env_idx, obs)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 109, in _save_obs\n    self.buf_obs[key][env_idx] = obs\nValueError: could\
    \ not broadcast input array from shape (49,) into shape (38,)\n"
  training_time: 3.49675
- error: could not broadcast input array from shape (39,) into shape (50,)
  model_id: model_003
  model_path: null
  params:
    algorithm: ppo
    batch_size: 128
    clip_range: 0.3
    ent_coef: 0.0
    gamma: 0.99
    initial_capital: 200000
    learning_rate: 0.0005
    model_id: model_003
    n_steps: 4096
    name: ppo_model_003
    reward_scaling: 1e-4
    tickers: &id002
    - AAPL
    - MSFT
    - NVDA
    - GOOGL
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 311, in learn\n    total_timesteps, callback = self._setup_learn(\n  File\
    \ \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py\"\
    , line 423, in _setup_learn\n    self._last_obs = self.env.reset()  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 79, in reset\n    self._save_obs(env_idx, obs)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 109, in _save_obs\n    self.buf_obs[key][env_idx] = obs\nValueError: could\
    \ not broadcast input array from shape (39,) into shape (50,)\n"
  training_time: 3.240585
- error: can't multiply sequence by non-int of type 'numpy.float64'
  model_id: model_004
  model_path: null
  params:
    algorithm: ppo
    batch_size: 32
    clip_range: 0.1
    ent_coef: 0.0
    gamma: 0.99
    initial_capital: 50000
    learning_rate: 0.0001
    model_id: model_004
    n_steps: 2048
    name: ppo_model_004
    reward_scaling: 1e-5
    tickers: &id004
    - GOOGL
    - NVDA
    - AMD
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 324, in learn\n    continue_training = self.collect_rollouts(self.env,\
    \ callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 218, in collect_rollouts\n    new_obs, rewards, dones, infos = env.step(clipped_actions)\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\
    , line 222, in step\n    return self.step_wait()\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 59, in step_wait\n    obs, self.buf_rews[env_idx], terminated, truncated,\
    \ self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/src/environment/trading_env.py\", line\
    \ 111, in step\n    reward = portfolio_return * self.reward_scaling\nTypeError:\
    \ can't multiply sequence by non-int of type 'numpy.float64'\n"
  training_time: 3.298974
- error: can't multiply sequence by non-int of type 'numpy.float64'
  model_id: model_005
  model_path: null
  params:
    algorithm: ppo
    batch_size: 128
    clip_range: 0.2
    ent_coef: 0.0
    gamma: 0.95
    initial_capital: 200000
    learning_rate: 0.0005
    model_id: model_005
    n_steps: 1024
    name: ppo_model_005
    reward_scaling: 1e-3
    tickers: &id003
    - SPY
    - QQQ
    - DIA
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 324, in learn\n    continue_training = self.collect_rollouts(self.env,\
    \ callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 218, in collect_rollouts\n    new_obs, rewards, dones, infos = env.step(clipped_actions)\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\
    , line 222, in step\n    return self.step_wait()\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 59, in step_wait\n    obs, self.buf_rews[env_idx], terminated, truncated,\
    \ self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/src/environment/trading_env.py\", line\
    \ 111, in step\n    reward = portfolio_return * self.reward_scaling\nTypeError:\
    \ can't multiply sequence by non-int of type 'numpy.float64'\n"
  training_time: 3.282937
- error: could not broadcast input array from shape (39,) into shape (50,)
  model_id: model_006
  model_path: null
  params:
    algorithm: ppo
    batch_size: 128
    clip_range: 0.3
    ent_coef: 0.01
    gamma: 0.995
    initial_capital: 50000
    learning_rate: 0.001
    model_id: model_006
    n_steps: 4096
    name: ppo_model_006
    reward_scaling: 1e-4
    tickers: &id001
    - JPM
    - BAC
    - WFC
    - GS
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 311, in learn\n    total_timesteps, callback = self._setup_learn(\n  File\
    \ \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py\"\
    , line 423, in _setup_learn\n    self._last_obs = self.env.reset()  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 79, in reset\n    self._save_obs(env_idx, obs)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 109, in _save_obs\n    self.buf_obs[key][env_idx] = obs\nValueError: could\
    \ not broadcast input array from shape (39,) into shape (50,)\n"
  training_time: 3.327352
- error: can't multiply sequence by non-int of type 'numpy.float64'
  model_id: model_007
  model_path: null
  params:
    algorithm: ppo
    batch_size: 32
    clip_range: 0.1
    ent_coef: 0.05
    gamma: 0.95
    initial_capital: 200000
    learning_rate: 0.0001
    model_id: model_007
    n_steps: 4096
    name: ppo_model_007
    reward_scaling: 1e-4
    tickers: *id001
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 324, in learn\n    continue_training = self.collect_rollouts(self.env,\
    \ callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 218, in collect_rollouts\n    new_obs, rewards, dones, infos = env.step(clipped_actions)\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\
    , line 222, in step\n    return self.step_wait()\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 59, in step_wait\n    obs, self.buf_rews[env_idx], terminated, truncated,\
    \ self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/src/environment/trading_env.py\", line\
    \ 111, in step\n    reward = portfolio_return * self.reward_scaling\nTypeError:\
    \ can't multiply sequence by non-int of type 'numpy.float64'\n"
  training_time: 3.645415
- error: could not broadcast input array from shape (39,) into shape (50,)
  model_id: model_008
  model_path: null
  params:
    algorithm: ppo
    batch_size: 128
    clip_range: 0.3
    ent_coef: 0.05
    gamma: 0.99
    initial_capital: 200000
    learning_rate: 0.0005
    model_id: model_008
    n_steps: 4096
    name: ppo_model_008
    reward_scaling: 1e-4
    tickers: *id001
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 311, in learn\n    total_timesteps, callback = self._setup_learn(\n  File\
    \ \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py\"\
    , line 423, in _setup_learn\n    self._last_obs = self.env.reset()  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 79, in reset\n    self._save_obs(env_idx, obs)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 109, in _save_obs\n    self.buf_obs[key][env_idx] = obs\nValueError: could\
    \ not broadcast input array from shape (39,) into shape (50,)\n"
  training_time: 3.399904
- error: can't multiply sequence by non-int of type 'numpy.float64'
  model_id: model_009
  model_path: null
  params:
    algorithm: ppo
    batch_size: 32
    clip_range: 0.3
    ent_coef: 0.0
    gamma: 0.995
    initial_capital: 100000
    learning_rate: 0.0005
    model_id: model_009
    n_steps: 4096
    name: ppo_model_009
    reward_scaling: 1e-3
    tickers: *id001
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 324, in learn\n    continue_training = self.collect_rollouts(self.env,\
    \ callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 218, in collect_rollouts\n    new_obs, rewards, dones, infos = env.step(clipped_actions)\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\
    , line 222, in step\n    return self.step_wait()\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 59, in step_wait\n    obs, self.buf_rews[env_idx], terminated, truncated,\
    \ self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/src/environment/trading_env.py\", line\
    \ 111, in step\n    reward = portfolio_return * self.reward_scaling\nTypeError:\
    \ can't multiply sequence by non-int of type 'numpy.float64'\n"
  training_time: 3.661967
- error: could not broadcast input array from shape (39,) into shape (50,)
  model_id: model_010
  model_path: null
  params:
    algorithm: ppo
    batch_size: 64
    clip_range: 0.3
    ent_coef: 0.01
    gamma: 0.99
    initial_capital: 100000
    learning_rate: 0.0005
    model_id: model_010
    n_steps: 4096
    name: ppo_model_010
    reward_scaling: 1e-3
    tickers: *id002
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 311, in learn\n    total_timesteps, callback = self._setup_learn(\n  File\
    \ \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/base_class.py\"\
    , line 423, in _setup_learn\n    self._last_obs = self.env.reset()  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 79, in reset\n    self._save_obs(env_idx, obs)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 109, in _save_obs\n    self.buf_obs[key][env_idx] = obs\nValueError: could\
    \ not broadcast input array from shape (39,) into shape (50,)\n"
  training_time: 3.401806
- error: can't multiply sequence by non-int of type 'numpy.float64'
  model_id: model_011
  model_path: null
  params:
    algorithm: ppo
    batch_size: 32
    clip_range: 0.2
    ent_coef: 0.0
    gamma: 0.95
    initial_capital: 200000
    learning_rate: 0.0001
    model_id: model_011
    n_steps: 4096
    name: ppo_model_011
    reward_scaling: 1e-4
    tickers: *id001
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 324, in learn\n    continue_training = self.collect_rollouts(self.env,\
    \ callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 218, in collect_rollouts\n    new_obs, rewards, dones, infos = env.step(clipped_actions)\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\
    , line 222, in step\n    return self.step_wait()\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 59, in step_wait\n    obs, self.buf_rews[env_idx], terminated, truncated,\
    \ self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/src/environment/trading_env.py\", line\
    \ 111, in step\n    reward = portfolio_return * self.reward_scaling\nTypeError:\
    \ can't multiply sequence by non-int of type 'numpy.float64'\n"
  training_time: 3.731668
- error: can't multiply sequence by non-int of type 'numpy.float64'
  model_id: model_012
  model_path: null
  params:
    algorithm: ppo
    batch_size: 32
    clip_range: 0.1
    ent_coef: 0.01
    gamma: 0.95
    initial_capital: 100000
    learning_rate: 0.001
    model_id: model_012
    n_steps: 1024
    name: ppo_model_012
    reward_scaling: 1e-5
    tickers:
    - XLF
    - XLK
    - XLE
    - XLV
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 324, in learn\n    continue_training = self.collect_rollouts(self.env,\
    \ callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 218, in collect_rollouts\n    new_obs, rewards, dones, infos = env.step(clipped_actions)\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\
    , line 222, in step\n    return self.step_wait()\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 59, in step_wait\n    obs, self.buf_rews[env_idx], terminated, truncated,\
    \ self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/src/environment/trading_env.py\", line\
    \ 111, in step\n    reward = portfolio_return * self.reward_scaling\nTypeError:\
    \ can't multiply sequence by non-int of type 'numpy.float64'\n"
  training_time: 3.695332
- error: can't multiply sequence by non-int of type 'numpy.float64'
  model_id: model_013
  model_path: null
  params:
    algorithm: ppo
    batch_size: 128
    clip_range: 0.3
    ent_coef: 0.05
    gamma: 0.995
    initial_capital: 100000
    learning_rate: 0.0001
    model_id: model_013
    n_steps: 2048
    name: ppo_model_013
    reward_scaling: 1e-5
    tickers:
    - AAPL
    - NVDA
    - TSLA
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 324, in learn\n    continue_training = self.collect_rollouts(self.env,\
    \ callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 218, in collect_rollouts\n    new_obs, rewards, dones, infos = env.step(clipped_actions)\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\
    , line 222, in step\n    return self.step_wait()\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 59, in step_wait\n    obs, self.buf_rews[env_idx], terminated, truncated,\
    \ self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/src/environment/trading_env.py\", line\
    \ 111, in step\n    reward = portfolio_return * self.reward_scaling\nTypeError:\
    \ can't multiply sequence by non-int of type 'numpy.float64'\n"
  training_time: 3.424012
- error: can't multiply sequence by non-int of type 'numpy.float64'
  model_id: model_014
  model_path: null
  params:
    algorithm: ppo
    batch_size: 128
    clip_range: 0.2
    ent_coef: 0.01
    gamma: 0.995
    initial_capital: 200000
    learning_rate: 0.002
    model_id: model_014
    n_steps: 1024
    name: ppo_model_014
    reward_scaling: 1e-4
    tickers: *id003
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 324, in learn\n    continue_training = self.collect_rollouts(self.env,\
    \ callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 218, in collect_rollouts\n    new_obs, rewards, dones, infos = env.step(clipped_actions)\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\
    , line 222, in step\n    return self.step_wait()\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 59, in step_wait\n    obs, self.buf_rews[env_idx], terminated, truncated,\
    \ self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/src/environment/trading_env.py\", line\
    \ 111, in step\n    reward = portfolio_return * self.reward_scaling\nTypeError:\
    \ can't multiply sequence by non-int of type 'numpy.float64'\n"
  training_time: 3.333578
- error: can't multiply sequence by non-int of type 'numpy.float64'
  model_id: model_015
  model_path: null
  params:
    algorithm: ppo
    batch_size: 64
    clip_range: 0.3
    ent_coef: 0.0
    gamma: 0.95
    initial_capital: 100000
    learning_rate: 0.0005
    model_id: model_015
    n_steps: 4096
    name: ppo_model_015
    reward_scaling: 1e-3
    tickers: *id004
    timesteps: 1500000
  success: false
  traceback: "Traceback (most recent call last):\n  File \"/home/salt/CodingProjects/RLFI/src/autotest/automated_trainer.py\"\
    , line 159, in train_model\n    model = trainer.train(total_timesteps=params['timesteps'],\
    \ eval_env=eval_env)\n  File \"/home/salt/CodingProjects/RLFI/src/agents/trainer.py\"\
    , line 111, in train\n    self.model.learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/ppo/ppo.py\"\
    , line 311, in learn\n    return super().learn(\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 324, in learn\n    continue_training = self.collect_rollouts(self.env,\
    \ callback, self.rollout_buffer, n_rollout_steps=self.n_steps)\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py\"\
    , line 218, in collect_rollouts\n    new_obs, rewards, dones, infos = env.step(clipped_actions)\n\
    \  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/base_vec_env.py\"\
    , line 222, in step\n    return self.step_wait()\n  File \"/home/salt/CodingProjects/RLFI/venv/lib/python3.10/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py\"\
    , line 59, in step_wait\n    obs, self.buf_rews[env_idx], terminated, truncated,\
    \ self.buf_infos[env_idx] = self.envs[env_idx].step(  # type: ignore[assignment]\n\
    \  File \"/home/salt/CodingProjects/RLFI/src/environment/trading_env.py\", line\
    \ 111, in step\n    reward = portfolio_return * self.reward_scaling\nTypeError:\
    \ can't multiply sequence by non-int of type 'numpy.float64'\n"
  training_time: 3.388942
successful: 0
total_models: 15
